{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loss_train:  [[ 35835.52816047]]    loss_val:  [[ 14804.32536178]]\n",
      "   loss_train:  [[ 17644.79049226]]    loss_val:  [[ 7423.33540835]]\n",
      "   loss_train:  [[ 12746.06598904]]    loss_val:  [[ 4895.47641806]]\n",
      "   loss_train:  [[ 10988.47491002]]    loss_val:  [[ 4292.26076292]]\n",
      "   loss_train:  [[ 10042.38326939]]    loss_val:  [[ 3836.41404151]]\n",
      "   loss_train:  [[ 9360.7099794]]    loss_val:  [[ 3581.82554834]]\n",
      "   loss_train:  [[ 8805.86814105]]    loss_val:  [[ 3338.78736827]]\n",
      "   loss_train:  [[ 8336.01080757]]    loss_val:  [[ 3144.43947426]]\n",
      "   loss_train:  [[ 7932.92304582]]    loss_val:  [[ 2968.83133184]]\n",
      "   loss_train:  [[ 7585.21813327]]    loss_val:  [[ 2817.50901973]]\n",
      "   loss_train:  [[ 7284.24547268]]    loss_val:  [[ 2683.464394]]\n",
      "   loss_train:  [[ 7022.95340178]]    loss_val:  [[ 2565.81741379]]\n",
      "   loss_train:  [[ 6795.46438347]]    loss_val:  [[ 2461.81923385]]\n",
      "   loss_train:  [[ 6596.84231166]]    loss_val:  [[ 2369.9210682]]\n",
      "   loss_train:  [[ 6422.92746057]]    loss_val:  [[ 2288.48929102]]\n",
      "   loss_train:  [[ 6270.20628759]]    loss_val:  [[ 2216.23174394]]\n",
      "   loss_train:  [[ 6135.70510536]]    loss_val:  [[ 2151.9971945]]\n",
      "   loss_train:  [[ 6016.90227717]]    loss_val:  [[ 2094.80280562]]\n",
      "   loss_train:  [[ 5911.65540162]]    loss_val:  [[ 2043.7961588]]\n",
      "   loss_train:  [[ 5818.14078844]]    loss_val:  [[ 1998.23785423]]\n",
      "   loss_train:  [[ 5734.80306164]]    loss_val:  [[ 1957.48650925]]\n",
      "   loss_train:  [[ 5660.31312666]]    loss_val:  [[ 1920.98322541]]\n",
      "   loss_train:  [[ 5593.53305704]]    loss_val:  [[ 1888.2411527]]\n",
      "   loss_train:  [[ 5533.4867143]]    loss_val:  [[ 1858.83471218]]\n",
      "   loss_train:  [[ 5479.33512515]]    loss_val:  [[ 1832.3916237]]\n",
      "   loss_train:  [[ 5430.35581203]]    loss_val:  [[ 1808.58541327]]\n",
      "   loss_train:  [[ 5385.92541378]]    loss_val:  [[ 1787.1294047]]\n",
      "   loss_train:  [[ 5345.50504857]]    loss_val:  [[ 1767.7713663]]\n",
      "   loss_train:  [[ 5308.62796598]]    loss_val:  [[ 1750.28903483]]\n",
      "   loss_train:  [[ 5274.88911277]]    loss_val:  [[ 1734.48621201]]\n",
      "   loss_train:  [[ 5243.93630114]]    loss_val:  [[ 1720.1894366]]\n",
      "   loss_train:  [[ 5215.46272078]]    loss_val:  [[ 1707.24510033]]\n",
      "   loss_train:  [[ 5189.20057955]]    loss_val:  [[ 1695.51696551]]\n",
      "   loss_train:  [[ 5164.91569384]]    loss_val:  [[ 1684.88401401]]\n",
      "   loss_train:  [[ 5142.40287887]]    loss_val:  [[ 1675.23858559]]\n",
      "   loss_train:  [[ 5121.48201433]]    loss_val:  [[ 1666.48476131]]\n",
      "   loss_train:  [[ 5101.99468062]]    loss_val:  [[ 1658.53695885]]\n",
      "   loss_train:  [[ 5083.80127848]]    loss_val:  [[ 1651.31870929]]\n",
      "   loss_train:  [[ 5066.77855827]]    loss_val:  [[ 1644.76159066]]\n",
      "   loss_train:  [[ 5050.81749734]]    loss_val:  [[ 1638.80429651]]\n",
      "   loss_train:  [[ 5035.82147349]]    loss_val:  [[ 1633.39182126]]\n",
      "   loss_train:  [[ 5021.70469072]]    loss_val:  [[ 1628.47474673]]\n",
      "   loss_train:  [[ 5008.39082021]]    loss_val:  [[ 1624.00861621]]\n",
      "   loss_train:  [[ 4995.81182546]]    loss_val:  [[ 1619.95338466]]\n",
      "   loss_train:  [[ 4983.90694486]]    loss_val:  [[ 1616.27293514]]\n",
      "   loss_train:  [[ 4972.62180938]]    loss_val:  [[ 1612.93465276]]\n",
      "   loss_train:  [[ 4961.90767629]]    loss_val:  [[ 1609.90904902]]\n",
      "   loss_train:  [[ 4951.72076255]]    loss_val:  [[ 1607.16942992]]\n",
      "   loss_train:  [[ 4942.02166421]]    loss_val:  [[ 1604.6916025]]\n",
      "   loss_train:  [[ 4932.77484987]]    loss_val:  [[ 1602.45361505]]\n",
      "   loss_train:  [[ 4923.94821816]]    loss_val:  [[ 1600.43552669]]\n",
      "   loss_train:  [[ 4915.51271057]]    loss_val:  [[ 1598.61920287]]\n",
      "   loss_train:  [[ 4907.44197234]]    loss_val:  [[ 1596.98813365]]\n",
      "   loss_train:  [[ 4899.71205485]]    loss_val:  [[ 1595.52727197]]\n",
      "   loss_train:  [[ 4892.30115422]]    loss_val:  [[ 1594.22288954]]\n",
      "   loss_train:  [[ 4885.18938135]]    loss_val:  [[ 1593.06244835]]\n",
      "   loss_train:  [[ 4878.35855932]]    loss_val:  [[ 1592.0344859]]\n",
      "   loss_train:  [[ 4871.79204466]]    loss_val:  [[ 1591.1285126]]\n",
      "   loss_train:  [[ 4865.47456951]]    loss_val:  [[ 1590.33491996]]\n",
      "   loss_train:  [[ 4859.392102]]    loss_val:  [[ 1589.64489827]]\n",
      "   loss_train:  [[ 4853.53172258]]    loss_val:  [[ 1589.05036286]]\n",
      "   loss_train:  [[ 4847.88151433]]    loss_val:  [[ 1588.54388771]]\n",
      "   loss_train:  [[ 4842.43046557]]    loss_val:  [[ 1588.11864599]]\n",
      "   loss_train:  [[ 4837.16838328]]    loss_val:  [[ 1587.76835631]]\n",
      "   loss_train:  [[ 4832.0858159]]    loss_val:  [[ 1587.4872345]]\n",
      "   loss_train:  [[ 4827.17398468]]    loss_val:  [[ 1587.26994996]]\n",
      "   loss_train:  [[ 4822.42472217]]    loss_val:  [[ 1587.11158637]]\n",
      "   loss_train:  [[ 4817.83041741]]    loss_val:  [[ 1587.00760608]]\n",
      "   loss_train:  [[ 4813.38396677]]    loss_val:  [[ 1586.95381799]]\n",
      "   loss_train:  [[ 4809.07872988]]    loss_val:  [[ 1586.94634839]]\n",
      "   loss_train:  [[ 4804.9084901]]    loss_val:  [[ 1586.98161451]]\n",
      "   loss_train:  [[ 4800.86741901]]    loss_val:  [[ 1587.05630062]]\n",
      "   loss_train:  [[ 4796.95004434]]    loss_val:  [[ 1587.16733618]]\n",
      "   loss_train:  [[ 4793.15122118]]    loss_val:  [[ 1587.3118761]]\n",
      "   loss_train:  [[ 4789.46610596]]    loss_val:  [[ 1587.48728268]]\n",
      "   loss_train:  [[ 4785.89013287]]    loss_val:  [[ 1587.69110918]]\n",
      "   loss_train:  [[ 4782.41899259]]    loss_val:  [[ 1587.92108489]]\n",
      "   loss_train:  [[ 4779.04861299]]    loss_val:  [[ 1588.17510144]]\n",
      "   loss_train:  [[ 4775.77514159]]    loss_val:  [[ 1588.45120028]]\n",
      "   loss_train:  [[ 4772.59492962]]    loss_val:  [[ 1588.74756132]]\n",
      "   loss_train:  [[ 4769.50451755]]    loss_val:  [[ 1589.06249241]]\n",
      "   loss_train:  [[ 4766.50062187]]    loss_val:  [[ 1589.39441982]]\n",
      "   loss_train:  [[ 4763.58012299]]    loss_val:  [[ 1589.74187944]]\n",
      "   loss_train:  [[ 4760.74005424]]    loss_val:  [[ 1590.10350868]]\n",
      "   loss_train:  [[ 4757.97759177]]    loss_val:  [[ 1590.47803912]]\n",
      "   loss_train:  [[ 4755.2900453]]    loss_val:  [[ 1590.8642897]]\n",
      "   loss_train:  [[ 4752.67484961]]    loss_val:  [[ 1591.26116042]]\n",
      "   loss_train:  [[ 4750.12955674]]    loss_val:  [[ 1591.66762659]]\n",
      "   loss_train:  [[ 4747.65182879]]    loss_val:  [[ 1592.08273353]]\n",
      "   loss_train:  [[ 4745.23943131]]    loss_val:  [[ 1592.50559165]]\n",
      "   loss_train:  [[ 4742.89022719]]    loss_val:  [[ 1592.93537194]]\n",
      "   loss_train:  [[ 4740.60217098]]    loss_val:  [[ 1593.37130176]]\n",
      "   loss_train:  [[ 4738.37330368]]    loss_val:  [[ 1593.81266101]]\n",
      "   loss_train:  [[ 4736.20174789]]    loss_val:  [[ 1594.25877856]]\n",
      "   loss_train:  [[ 4734.08570329]]    loss_val:  [[ 1594.70902887]]\n",
      "   loss_train:  [[ 4732.0234425]]    loss_val:  [[ 1595.16282903]]\n",
      "   loss_train:  [[ 4730.01330718]]    loss_val:  [[ 1595.61963583]]\n",
      "   loss_train:  [[ 4728.05370438]]    loss_val:  [[ 1596.07894315]]\n",
      "   loss_train:  [[ 4726.14310321]]    loss_val:  [[ 1596.54027953]]\n",
      "   loss_train:  [[ 4724.28003165]]    loss_val:  [[ 1597.00320589]]\n",
      "   loss_train:  [[ 4722.46307364]]    loss_val:  [[ 1597.46731337]]\n",
      "   loss_train:  [[ 4720.6908663]]    loss_val:  [[ 1597.93222145]]\n",
      "   loss_train:  [[ 4718.96209734]]    loss_val:  [[ 1598.39757604]]\n",
      "   loss_train:  [[ 4717.27550267]]    loss_val:  [[ 1598.8630478]]\n",
      "   loss_train:  [[ 4715.62986413]]    loss_val:  [[ 1599.32833059]]\n",
      "   loss_train:  [[ 4714.0240073]]    loss_val:  [[ 1599.79313993]]\n",
      "   loss_train:  [[ 4712.45679956]]    loss_val:  [[ 1600.25721166]]\n",
      "   loss_train:  [[ 4710.92714815]]    loss_val:  [[ 1600.72030066]]\n",
      "   loss_train:  [[ 4709.43399844]]    loss_val:  [[ 1601.1821796]]\n",
      "   loss_train:  [[ 4707.97633217]]    loss_val:  [[ 1601.64263785]]\n",
      "   loss_train:  [[ 4706.55316596]]    loss_val:  [[ 1602.10148045]]\n",
      "   loss_train:  [[ 4705.16354973]]    loss_val:  [[ 1602.55852707]]\n",
      "   loss_train:  [[ 4703.80656533]]    loss_val:  [[ 1603.01361115]]\n",
      "   loss_train:  [[ 4702.48132517]]    loss_val:  [[ 1603.46657901]]\n",
      "   loss_train:  [[ 4701.18697097]]    loss_val:  [[ 1603.91728904]]\n",
      "   loss_train:  [[ 4699.92267256]]    loss_val:  [[ 1604.36561098]]\n",
      "   loss_train:  [[ 4698.68762671]]    loss_val:  [[ 1604.81142518]]\n",
      "   loss_train:  [[ 4697.48105606]]    loss_val:  [[ 1605.25462194]]\n",
      "   loss_train:  [[ 4696.3022081]]    loss_val:  [[ 1605.69510093]]\n",
      "   loss_train:  [[ 4695.15035418]]    loss_val:  [[ 1606.13277055]]\n",
      "   loss_train:  [[ 4694.02478858]]    loss_val:  [[ 1606.56754744]]\n",
      "   loss_train:  [[ 4692.92482761]]    loss_val:  [[ 1606.99935592]]\n",
      "   loss_train:  [[ 4691.8498088]]    loss_val:  [[ 1607.42812755]]\n",
      "   loss_train:  [[ 4690.79909008]]    loss_val:  [[ 1607.85380064]]\n",
      "   loss_train:  [[ 4689.77204897]]    loss_val:  [[ 1608.27631986]]\n",
      "   loss_train:  [[ 4688.76808194]]    loss_val:  [[ 1608.69563582]]\n",
      "   loss_train:  [[ 4687.78660363]]    loss_val:  [[ 1609.1117047]]\n",
      "   loss_train:  [[ 4686.82704621]]    loss_val:  [[ 1609.52448792]]\n",
      "   loss_train:  [[ 4685.88885876]]    loss_val:  [[ 1609.93395176]]\n",
      "   loss_train:  [[ 4684.97150664]]    loss_val:  [[ 1610.34006711]]\n",
      "   loss_train:  [[ 4684.07447091]]    loss_val:  [[ 1610.74280912]]\n",
      "   loss_train:  [[ 4683.1972478]]    loss_val:  [[ 1611.14215696]]\n",
      "   loss_train:  [[ 4682.33934815]]    loss_val:  [[ 1611.53809357]]\n",
      "   loss_train:  [[ 4681.50029688]]    loss_val:  [[ 1611.93060536]]\n",
      "   loss_train:  [[ 4680.67963257]]    loss_val:  [[ 1612.31968206]]\n",
      "   loss_train:  [[ 4679.87690691]]    loss_val:  [[ 1612.70531644]]\n",
      "   loss_train:  [[ 4679.09168433]]    loss_val:  [[ 1613.08750414]]\n",
      "   loss_train:  [[ 4678.32354149]]    loss_val:  [[ 1613.46624345]]\n",
      "   loss_train:  [[ 4677.57206694]]    loss_val:  [[ 1613.84153519]]\n",
      "   loss_train:  [[ 4676.83686068]]    loss_val:  [[ 1614.21338246]]\n",
      "   loss_train:  [[ 4676.11753378]]    loss_val:  [[ 1614.58179053]]\n",
      "   loss_train:  [[ 4675.41370805]]    loss_val:  [[ 1614.9467667]]\n",
      "   loss_train:  [[ 4674.72501564]]    loss_val:  [[ 1615.30832011]]\n",
      "   loss_train:  [[ 4674.05109876]]    loss_val:  [[ 1615.66646166]]\n",
      "   loss_train:  [[ 4673.39160928]]    loss_val:  [[ 1616.02120384]]\n",
      "   loss_train:  [[ 4672.7462085]]    loss_val:  [[ 1616.37256065]]\n",
      "   loss_train:  [[ 4672.11456681]]    loss_val:  [[ 1616.72054745]]\n",
      "   loss_train:  [[ 4671.49636337]]    loss_val:  [[ 1617.0651809]]\n",
      "   loss_train:  [[ 4670.89128592]]    loss_val:  [[ 1617.40647881]]\n",
      "   loss_train:  [[ 4670.29903041]]    loss_val:  [[ 1617.74446011]]\n",
      "   loss_train:  [[ 4669.71930084]]    loss_val:  [[ 1618.07914469]]\n",
      "   loss_train:  [[ 4669.15180892]]    loss_val:  [[ 1618.41055338]]\n",
      "   loss_train:  [[ 4668.59627391]]    loss_val:  [[ 1618.73870785]]\n",
      "   loss_train:  [[ 4668.05242236]]    loss_val:  [[ 1619.06363051]]\n",
      "   loss_train:  [[ 4667.51998785]]    loss_val:  [[ 1619.38534449]]\n",
      "   loss_train:  [[ 4666.99871085]]    loss_val:  [[ 1619.70387355]]\n",
      "   loss_train:  [[ 4666.48833847]]    loss_val:  [[ 1620.01924202]]\n",
      "   loss_train:  [[ 4665.98862425]]    loss_val:  [[ 1620.33147473]]\n",
      "   loss_train:  [[ 4665.499328]]    loss_val:  [[ 1620.640597]]\n",
      "   loss_train:  [[ 4665.0202156]]    loss_val:  [[ 1620.94663454]]\n",
      "   loss_train:  [[ 4664.55105884]]    loss_val:  [[ 1621.24961343]]\n",
      "   loss_train:  [[ 4664.09163519]]    loss_val:  [[ 1621.54956007]]\n",
      "   loss_train:  [[ 4663.64172772]]    loss_val:  [[ 1621.84650114]]\n",
      "   loss_train:  [[ 4663.20112486]]    loss_val:  [[ 1622.14046357]]\n",
      "   loss_train:  [[ 4662.7696203]]    loss_val:  [[ 1622.43147448]]\n",
      "   loss_train:  [[ 4662.34701281]]    loss_val:  [[ 1622.71956115]]\n",
      "   loss_train:  [[ 4661.93310611]]    loss_val:  [[ 1623.00475102]]\n",
      "   loss_train:  [[ 4661.52770872]]    loss_val:  [[ 1623.28707162]]\n",
      "   loss_train:  [[ 4661.13063383]]    loss_val:  [[ 1623.56655056]]\n",
      "   loss_train:  [[ 4660.74169917]]    loss_val:  [[ 1623.84321551]]\n",
      "   loss_train:  [[ 4660.36072688]]    loss_val:  [[ 1624.11709416]]\n",
      "   loss_train:  [[ 4659.98754339]]    loss_val:  [[ 1624.38821422]]\n",
      "   loss_train:  [[ 4659.6219793]]    loss_val:  [[ 1624.65660336]]\n",
      "   loss_train:  [[ 4659.26386926]]    loss_val:  [[ 1624.92228922]]\n",
      "   loss_train:  [[ 4658.91305188]]    loss_val:  [[ 1625.18529941]]\n",
      "   loss_train:  [[ 4658.56936959]]    loss_val:  [[ 1625.44566142]]\n",
      "   loss_train:  [[ 4658.23266857]]    loss_val:  [[ 1625.7034027]]\n",
      "   loss_train:  [[ 4657.90279862]]    loss_val:  [[ 1625.95855056]]\n",
      "   loss_train:  [[ 4657.57961309]]    loss_val:  [[ 1626.21113221]]\n",
      "   loss_train:  [[ 4657.26296875]]    loss_val:  [[ 1626.46117473]]\n",
      "   loss_train:  [[ 4656.95272574]]    loss_val:  [[ 1626.70870506]]\n",
      "   loss_train:  [[ 4656.64874744]]    loss_val:  [[ 1626.95374996]]\n",
      "   loss_train:  [[ 4656.35090043]]    loss_val:  [[ 1627.19633607]]\n",
      "   loss_train:  [[ 4656.05905436]]    loss_val:  [[ 1627.43648981]]\n",
      "   loss_train:  [[ 4655.77308189]]    loss_val:  [[ 1627.67423747]]\n",
      "   loss_train:  [[ 4655.49285864]]    loss_val:  [[ 1627.90960511]]\n",
      "   loss_train:  [[ 4655.21826306]]    loss_val:  [[ 1628.14261861]]\n",
      "   loss_train:  [[ 4654.94917638]]    loss_val:  [[ 1628.37330364]]\n",
      "   loss_train:  [[ 4654.68548255]]    loss_val:  [[ 1628.60168567]]\n",
      "   loss_train:  [[ 4654.42706817]]    loss_val:  [[ 1628.82778997]]\n",
      "   loss_train:  [[ 4654.1738224]]    loss_val:  [[ 1629.05164155]]\n",
      "   loss_train:  [[ 4653.92563692]]    loss_val:  [[ 1629.27326523]]\n",
      "   loss_train:  [[ 4653.68240585]]    loss_val:  [[ 1629.4926856]]\n",
      "   loss_train:  [[ 4653.44402568]]    loss_val:  [[ 1629.70992702]]\n",
      "   loss_train:  [[ 4653.21039525]]    loss_val:  [[ 1629.92501359]]\n",
      "   loss_train:  [[ 4652.98141565]]    loss_val:  [[ 1630.13796922]]\n",
      "   loss_train:  [[ 4652.75699019]]    loss_val:  [[ 1630.34881754]]\n",
      "   loss_train:  [[ 4652.53702431]]    loss_val:  [[ 1630.55758198]]\n",
      "   loss_train:  [[ 4652.32142559]]    loss_val:  [[ 1630.76428568]]\n",
      "   loss_train:  [[ 4652.11010362]]    loss_val:  [[ 1630.96895159]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEREJJICQkREIikgREQk0v8Hn7OmIBOKCgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dd254ef908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import jupyter\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "\n",
    "def get_data():\n",
    "    data = load_svmlight_file(\"Housing.txt\")\n",
    "    return data[0], data[1]\n",
    "\n",
    "# 计算损失函数\n",
    "def cost_fn(x, y, heta):\n",
    "    return 0.5 * (y - x.dot(heta)).transpose().dot((y - x.dot(heta)))\n",
    "    #eturn np.sum((x.dot(heta) - y) ** 2) / (2 * m)\n",
    "\n",
    "# 计算梯度\n",
    "def g(x, y, heta):\n",
    "    return np.dot(X_train.transpose(), np.dot(X_train, heta)) - np.dot(X_train.transpose(), y_train)\n",
    "     #return np.array((y_pred(X_train,theta)- y).dot(x)).reshape(theta) / m\n",
    "\n",
    "    \n",
    "X, y = get_data()\n",
    "X = X.toarray()\n",
    "m = y.shape[0]\n",
    "y=y.reshape(m,1)\n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "  \n",
    "    \n",
    "heta = np.random.random(size = (X_train.shape[1], 1))#初始化\n",
    "alpha = 0.0011#学习率\n",
    " # 迭代的次数   \n",
    "iteration = 200 \n",
    "\n",
    "Train = []\n",
    "Val = []\n",
    "\n",
    "# 梯度下降法更新参数\n",
    "for i in range(iteration):\n",
    "    #更新参数\n",
    "    heta = heta - a * (g(X_train,y_train,heta))\n",
    "    loss_train = (y_train - X_train.dot(heta)).transpose().dot((y_train - X_train.dot(heta)))/2\n",
    "    loss_val = (y_val-X_val.dot(heta)).transpose().dot((y_val - X_val.dot(heta)))/2\n",
    "    Train.append(loss_train[0] /  X_train.shape[0])\n",
    "    Val.append(loss_val[0] /  X_val.shape[0])\n",
    "    print( '   loss_train: ',loss_train, '   loss_val: ',loss_val)\n",
    "    \n",
    "plt.plot(Train,label='Train ')\n",
    "plt.plot(Val, label='Val')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
